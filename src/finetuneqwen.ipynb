{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12fb5cc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "  \u001b[31m×\u001b[0m Failed to build `xformers==0.0.30`\n",
      "\u001b[31m  ├─▶ \u001b[0mThe build backend returned an error\n",
      "\u001b[31m  ╰─▶ \u001b[0mCall to `setuptools.build_meta:__legacy__.build_wheel` failed (exit\n",
      "\u001b[31m      \u001b[0mstatus: 1)\n",
      "\n",
      "\u001b[31m      \u001b[0m\u001b[31m[stderr]\u001b[39m\n",
      "\u001b[31m      \u001b[0mTraceback (most recent call last):\n",
      "\u001b[31m      \u001b[0m  File \"<string>\", line 14, in <module>\n",
      "\u001b[31m      \u001b[0m  File\n",
      "\u001b[31m      \u001b[0m\"/Users/pavankumartaddi/.cache/uv/builds-v0/.tmpvs91DB/lib/python3.11/site-packages/setuptools/build_meta.py\",\n",
      "\u001b[31m      \u001b[0mline 331, in get_requires_for_build_wheel\n",
      "\u001b[31m      \u001b[0m    return self._get_build_requires(config_settings, requirements=[])\n",
      "\u001b[31m      \u001b[0m           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[31m      \u001b[0m  File\n",
      "\u001b[31m      \u001b[0m\"/Users/pavankumartaddi/.cache/uv/builds-v0/.tmpvs91DB/lib/python3.11/site-packages/setuptools/build_meta.py\",\n",
      "\u001b[31m      \u001b[0mline 301, in _get_build_requires\n",
      "\u001b[31m      \u001b[0m    self.run_setup()\n",
      "\u001b[31m      \u001b[0m  File\n",
      "\u001b[31m      \u001b[0m\"/Users/pavankumartaddi/.cache/uv/builds-v0/.tmpvs91DB/lib/python3.11/site-packages/setuptools/build_meta.py\",\n",
      "\u001b[31m      \u001b[0mline 512, in run_setup\n",
      "\u001b[31m      \u001b[0m    super().run_setup(setup_script=setup_script)\n",
      "\u001b[31m      \u001b[0m  File\n",
      "\u001b[31m      \u001b[0m\"/Users/pavankumartaddi/.cache/uv/builds-v0/.tmpvs91DB/lib/python3.11/site-packages/setuptools/build_meta.py\",\n",
      "\u001b[31m      \u001b[0mline 317, in run_setup\n",
      "\u001b[31m      \u001b[0m    exec(code, locals())\n",
      "\u001b[31m      \u001b[0m  File \"<string>\", line 24, in <module>\n",
      "\u001b[31m      \u001b[0mModuleNotFoundError: No module named 'torch'\n",
      "\n",
      "\u001b[31m      \u001b[0m\u001b[36m\u001b[1mhint\u001b[0m\u001b[39m\u001b[1m:\u001b[0m This error likely indicates that `\u001b[36mxformers@0.0.30\u001b[39m` depends\n",
      "\u001b[31m      \u001b[0mon `\u001b[36mtorch\u001b[39m`, but doesn't declare it as a build dependency. If\n",
      "\u001b[31m      \u001b[0m`\u001b[36mxformers\u001b[39m` is a first-party package, consider adding `\u001b[36mtorch\u001b[39m` to its\n",
      "\u001b[31m      \u001b[0m`\u001b[32mbuild-system.requires\u001b[39m`. Otherwise, `\u001b[32muv pip install torch\u001b[39m` into the\n",
      "\u001b[31m      \u001b[0menvironment and re-run with `\u001b[32m--no-build-isolation\u001b[39m`.\n",
      "\u001b[36m  help: \u001b[0m`\u001b[36mxformers\u001b[39m` (\u001b[36mv0.0.30\u001b[39m) was included because `\u001b[36munsloth\u001b[39m` (\u001b[36mv2025.5.1\u001b[39m)\n",
      "        depends on `\u001b[36mxformers\u001b[39m`\n"
     ]
    }
   ],
   "source": [
    "!pip install uv -qU\n",
    "!uv pip install unsloth tensorboard -qU --system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "180fd036",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n",
    "os.environ[\"HF_HOME\"] = \"./\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5057615e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'huggingface_hub'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhuggingface_hub\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HfFolder, login\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m HfFolder\u001b[38;5;241m.\u001b[39mget_token() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m      3\u001b[0m   login()\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'huggingface_hub'"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import HfFolder, login\n",
    "if HfFolder.get_token() is None:\n",
    "  login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11a76f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"Qwen/Qwen3-8b\"\n",
    "max_seq_length = 8192\n",
    "dtype  = None\n",
    "load_in_4bit = False\n",
    "load_in_8bit = False\n",
    "datasets = \"qwen3_finetune_dataset.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a708f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc, inspect, sys\n",
    "def clear_old_model_refs():\n",
    "  frm = inspect.currentframe().f_back\n",
    "  caller_locals = frm.f_locals\n",
    "  caller_globals = frm.f_globals\n",
    "  for var in (\"teacher\", \"model\", \"tokenizer\"):\n",
    "    if var in caller_locals:\n",
    "      try:\n",
    "        del caller_locals [var]\n",
    "        if var in sys.modules: \n",
    "          del sys.modules [var]\n",
    "        print(f\"deleted local {var}\")\n",
    "      except Exception as e:\n",
    "        print(f\"could not delete local (var): {e}\")\n",
    "\n",
    "    if var in caller_globals:\n",
    "      try:\n",
    "        del caller_globals [var]\n",
    "        print(f\"deleted global (var)\")\n",
    "      except Exception as e:\n",
    "        print(f\"could not delete global (var): {e}\")\n",
    "  gc.collect()\n",
    "  torch.cuda.empty_cache()\n",
    "  print(\"GPIL cache cleared\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502e9820",
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_old_model_refs()\n",
    "model,tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = model,\n",
    "    max_seq_length = max_seq_length,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    load_in_8bit = load_in_8bit,\n",
    "    token = \"\"\n",
    "\n",
    ")\n",
    "print(tokenizer.padding_side)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d925ed02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def unwrap_to_base(model):\n",
    "    \"\"\"Return the module that actually owns the decoder layers' list.\"\"\"\n",
    "    CANDIDATE_ATTRS = (\n",
    "        \"model\",\n",
    "        \"Language model\",\n",
    "        \"transformer\",\n",
    "        \"gpt_neox\",\n",
    "        \"backbone\",\n",
    "        \"base model\",\n",
    "    )\n",
    "\n",
    "    seen = set()\n",
    "    queue = [model]\n",
    "\n",
    "    while queue:\n",
    "        m = queue.pop()\n",
    "        if id(m) in seen:\n",
    "            continue\n",
    "        seen.add(id(m))\n",
    "\n",
    "        if hasattr(m, \"layers\"):\n",
    "            return m\n",
    "\n",
    "        for attr in CANDIDATE_ATTRS:\n",
    "            if hasattr(m, attr):\n",
    "                queue.append(getattr(m, attr))\n",
    "\n",
    "    raise AttributeError(\"Could not find decoder layers - print(dir(model)) to inspect manually.\")\n",
    "\n",
    "\n",
    "def print_shapes_of_first_layer(model):\n",
    "    base_model = unwrap_to_base(model)\n",
    "    first_layer = base_model.layers[0] if hasattr(base_model, \"layers\") else None\n",
    "\n",
    "    if first_layer:\n",
    "        for name, param in first_layer.named_parameters():\n",
    "            print(f\"{name}: {param.shape}\")\n",
    "    else:\n",
    "        print(\"No layers found in the model.\")\n",
    "\n",
    "\n",
    "def visualize_matrix_dimensions(model_name='Qwen3-8B'):\n",
    "    model = torch.load(model_name)\n",
    "    param_shapes = {name: param.shape for name, param in model.named_parameters()}\n",
    "\n",
    "    # Extract matrix dimensions\n",
    "    matrix_dims = [(name, shape) for name, shape in param_shapes.items() if len(shape) == 2]\n",
    "    matrix_dims.sort(key=lambda x: np.prod(x[1]), reverse=True)\n",
    "\n",
    "    # Plotting\n",
    "    fig, ax = plt.subplots(figsize=(12, 10))\n",
    "    y_labels = [name for name, _ in matrix_dims]\n",
    "    widths = [shape[1] for _, shape in matrix_dims]\n",
    "    heights = [shape[0] for _, shape in matrix_dims]\n",
    "\n",
    "    ax.barh(y_labels, widths, height=0.5, alpha=0.7, color='blue', label='Width')\n",
    "    ax.barh(y_labels, heights, height=0.25, alpha=0.7, color='orange', label='Height')\n",
    "\n",
    "    ax.set_xlabel(\"Dimensions\")\n",
    "    ax.set_title(\"Matrix Dimensions of Qwen3-8B\")\n",
    "    ax.legend()\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "# model = torch.load(\"path/to/qwen3-8b.pth\")\n",
    "# print_shapes_of_first_layer(model)\n",
    "# visualize_matrix_dimensions(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dda870f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank = 16\n",
    "lora_alpha = 50\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = rank,\n",
    "    finetune_vision_layers = False,\n",
    "    finetune_language_layers = True,\n",
    "    finetune_attention_modules = True,\n",
    "    finetune_mlp_modules = True,\n",
    "    use_gradient_checkpointing = \"unsloth\",\n",
    "    full_finetuning = False,\n",
    "    random_state = 3407,\n",
    "    use_rslora = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5806e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints trainable parameters and modules in a PyTorch model.\n",
    "    Specifically useful for identifying LoRA adapter components.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    trainable_modules = set()\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "            # Extract module name from parameter path\n",
    "            module_name = \".\".join(name.split(\".\")[:-1])\n",
    "            trainable_modules.add(module_name)\n",
    "    \n",
    "    print(f\"Trainable params: {trainable_params:,} | All params: {all_param:,} | Trainable%: {100 * trainable_params / all_param:.4f}%\")\n",
    "    print(\"\\nTrainable modules:\")\n",
    "    for module in sorted(trainable_modules):\n",
    "        print(f\"- {module}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492fb6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math # Calculate training parameters\n",
    "learning_rate = 5e^-5\n",
    "per_device_train_batch_size  = 1\n",
    "gradient_accumulation_steps = 4\n",
    "epochs = 3\n",
    "total_steps = (len(datasets) // (per_device_train_batch_size * gradient_accumulation_steps)) * epochs\n",
    "warmup_steps = int(0.015 * total_steps)         # 1% of total steps for warmup\n",
    "anneal_start_step = int(0.5 * total_steps)     # 50% of total steps for annealing start\n",
    "\n",
    "# Print training configuration\n",
    "print(f\"Virtual batch size: {per_device_train_batch_size * gradient_accumulation_steps}\")\n",
    "print(f\"Total steps: {total_steps}\")\n",
    "print(f\"Warmup steps (rounded): {warmup_steps}\")\n",
    "print(f\"Annealing start step: {anneal_start_step}\")\n",
    "\n",
    "# Initialize optimizer\n",
    "from torch.optim import AdamW\n",
    "optimizer = AdamW(\n",
    "    model.parameters(),\n",
    "    lr=learning_rate,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "# Custom learning rate scheduler\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "def custom_cosine_scheduler(optimizer, num_warmup_steps, num_training_steps, anneal_start_step):\n",
    "    \"\"\"\n",
    "    Custom scheduler with 3 phases:\n",
    "    1. Linear warmup (first 1% of steps)\n",
    "    2. Constant max LR (until 50% of steps)\n",
    "    3. Cosine annealing decay (last 50% of steps)\n",
    "    \"\"\"\n",
    "    def lr_lambda(current_step):\n",
    "        # Warmup phase\n",
    "        if current_step < num_warmup_steps:\n",
    "            return float(current_step) / float(max(1, num_warmup_steps))\n",
    "        \n",
    "        # Constant phase\n",
    "        elif current_step < anneal_start_step:\n",
    "            return 1.0\n",
    "        \n",
    "        # Cosine annealing phase\n",
    "        else:\n",
    "            progress = (current_step - anneal_start_step) / \\\n",
    "                      (num_training_steps - anneal_start_step)\n",
    "            return max(0.0, 0.5 * (1.0 + math.cos(math.pi * progress)))\n",
    "    \n",
    "    return LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "# Create scheduler with cosine annealing\n",
    "scheduler = custom_cosine_scheduler(\n",
    "    optimizer,\n",
    "    num_warmup_steps=warmup_steps,\n",
    "    num_training_steps=total_steps,\n",
    "    anneal_start_step=anneal_start_step,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b79ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMPLATE = {\n",
    "  \"qwen\" : (\n",
    "    \"<|im_start|>user\\n\",\n",
    "    \"<|im_start|>assistant\\n\"\n",
    "  )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609ddb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## commom aargs\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "common_args = {\n",
    "\"per_device_train_batch_size\": per_device_train_batch_size,\n",
    "\"per_device_eval_batch_size\": per_device_train_batch_size,\n",
    "\"gradient_accumulation_steps\": gradient_accumulation_steps,\n",
    "\"num_train_epochs\": epochs,\n",
    "\"logging_strategy\": \" steps\",\n",
    "\"logging_dir\": f\"logs/{model.split('/')[-1]}\",\n",
    "\"eval_strategy\": \"steps\",\n",
    "\"logging_steps\": min(max(int(0.05* total_steps), 1),10),\n",
    "\"eval_steps\": max(int(0.1* total_steps), 1),\n",
    "\"bf16\": is_bfloat16_supported(),\n",
    "\"fp16\": not is_bfloat16_supported(),\n",
    "\"report_to\": \"tensorboard\",\n",
    "\"seed\": 3407,\n",
    "\"output_dir\": \"outputs\",\n",
    "\"gradient_checkpointing\": True,\n",
    "\"gradient_checkpointing_kwargs\": {\"use_reentrant\": True},\n",
    "\"remove_unused_columns\": True\n",
    "}\n",
    "common_args [\"dataset_num_proc\"] = 1\n",
    "common_args[\"dataset_text_field\"]= \"text\"\n",
    "common_args [\"max_seq_length\"] = max_seq_length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8ccc9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=ft_train_data,\n",
    "    eval_dataset=ft_eval_data,\n",
    "    args=TrainingArguments(**common_args),  # Convert dict to HF TrainingArguments\n",
    "\n",
    "    packing=False,\n",
    ")\n",
    "\n",
    "# ===== Critical Unsloth Integration Section =====\n",
    "# Manually inject custom optimizer and scheduler\n",
    "trainer.optimizer = optimizer  # Your preconfigured optimizer\n",
    "trainer.lr_scheduler = scheduler  # Your custom scheduler\n",
    "\n",
    "# Prevent TRL from reinitializing components\n",
    "trainer.create_optimizer = lambda *args, **kwargs: trainer.optimizer\n",
    "trainer.create_scheduler = lambda *args, **kwargs: trainer.lr_scheduler\n",
    "# ================================================\n",
    "\n",
    "# Now you can safely call trainer.train()\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb173fed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
